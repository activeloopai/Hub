{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imagenet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IGu8bprtI4Wi"
      },
      "source": [
        "# Benchmarks: Hub v/s WebDataset\n",
        "In this notebook we compare the packages Hub and WebDataset for their read timings. \n",
        "\n",
        "Both use a backend involving Sharded Datasets. For this experiment we will be using the ImageNet Dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4enEUBQJeiI"
      },
      "source": [
        "## Installing the Dependencies\n",
        "\n",
        "First of all, we gather all the dependencies as instructed by the repository \n",
        "\n",
        "[https://github.com/tmbdev/pytorch-imagenet-wds](https://github.com/tmbdev/pytorch-imagenet-wds)\n",
        "\n",
        "This will help us set up the environment for WebDataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL-dFFM_EPai"
      },
      "source": [
        "!pip install hub\n",
        "!pip install webdataset\n",
        "!pip install braceexpand\n",
        "!pip install numpy\n",
        "!pip install scipy\n",
        "!pip install tk\n",
        "!pip install matplotlib\n",
        "!pip install torch\n",
        "!pip install torchvision"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBuHzLosJ95Y"
      },
      "source": [
        "Now that the dependencies have been installed, we can focus on sharding the dataset for WebDataset. Following the instructions in the repository mentioned above, we have gotten the following script for sharding the ImageNet Dataset.\n",
        "\n",
        "There are a few parameters which can be adjusted accordingly to make the primary functions run without errors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1ebZDnhGLCih"
      },
      "source": [
        "import sys\n",
        "import os\n",
        "import os.path\n",
        "import random\n",
        "import argparse\n",
        "from torchvision import datasets\n",
        "import webdataset as wds\n",
        "import torch\n",
        "import torchvision\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6LXV7g9xElh6"
      },
      "source": [
        "'''Parameters (not attatched to parser)'''\n",
        "maxsize = 1e9\n",
        "maxcount = 1000\n",
        "filekey = False\n",
        "data = \"./data\"\n",
        "shards = \"./shards\"\n",
        "splits = \"train,val\"\n",
        "\n",
        "'''Original Code'''\n",
        "# parser = argparse.ArgumentParser(\"\"\"Generate sharded dataset from original ImageNet data.\"\"\")\n",
        "# parser.add_argument(\"--splits\", default=\"train,val\", help=\"which splits to write\")\n",
        "# parser.add_argument(\n",
        "#     \"--filekey\", action=\"store_true\", help=\"use file as key (default: index)\"\n",
        "# )\n",
        "# parser.add_argument(\"--maxsize\", type=float, default=1e9)\n",
        "# parser.add_argument(\"--maxcount\", type=float, default=1000)\n",
        "# parser.add_argument(\n",
        "#     \"--shards\", default=\"./shards\", help=\"directory where shards are written\"\n",
        "# )\n",
        "# parser.add_argument(\n",
        "#     \"--data\",\n",
        "#     default=\"./data\",\n",
        "#     help=\"directory containing ImageNet data distribution suitable for torchvision.datasets\",\n",
        "# )\n",
        "# args = parser.parse_args()\n",
        "\n",
        "\n",
        "# assert args.maxsize > 10000000\n",
        "# assert args.maxcount < 1000000\n",
        "\n",
        "\n",
        "\n",
        "if not os.path.isdir(os.path.join(data, \"train\")):\n",
        "    print(f\"{data}: should be directory containing ImageNet\", file=sys.stderr)\n",
        "    print(f\"suitable as argument for torchvision.datasets.ImageNet(...)\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "if not os.path.isdir(os.path.join(shards, \".\")):\n",
        "    print(f\"{shards}: should be a writable destination directory for shards\", file=sys.stderr)\n",
        "    sys.exit(1)\n",
        "\n",
        "\n",
        "splits = splits.split(\",\")\n",
        "\n",
        "\n",
        "def readfile(fname):\n",
        "    \"Read a binary file from disk.\"\n",
        "    with open(fname, \"rb\") as stream:\n",
        "        return stream.read()\n",
        "\n",
        "\n",
        "all_keys = set()\n",
        "\n",
        "\n",
        "def write_dataset(imagenet, base=\"./shards\", split=\"train\"):\n",
        "\n",
        "    # We're using the torchvision ImageNet dataset\n",
        "    # to parse the metadata; however, we will read\n",
        "    # the compressed images directly from disk (to\n",
        "    # avoid having to reencode them)\n",
        "    ds = datasets.ImageNet(imagenet, split=split)\n",
        "    nimages = len(ds.imgs)\n",
        "    print(\"# nimages\", nimages)\n",
        "\n",
        "    # We shuffle the indexes to make sure that we\n",
        "    # don't get any large sequences of a single class\n",
        "    # in the dataset.\n",
        "    indexes = list(range(nimages))\n",
        "    random.shuffle(indexes)\n",
        "\n",
        "    # This is the output pattern under which we write shards.\n",
        "    pattern = os.path.join(base, f\"imagenet-{split}-%06d.tar\")\n",
        "\n",
        "    with wds.ShardWriter(pattern, maxsize=int(maxsize), maxcount=int(maxcount)) as sink:\n",
        "        for i in indexes:\n",
        "\n",
        "            # Internal information from the ImageNet dataset\n",
        "            # instance: the file name and the numerical class.\n",
        "            fname, cls = ds.imgs[i]\n",
        "            assert cls == ds.targets[i]\n",
        "\n",
        "            # Read the JPEG-compressed image file contents.\n",
        "            image = readfile(fname)\n",
        "\n",
        "            # Construct a uniqu keye from the filename.\n",
        "            key = os.path.splitext(os.path.basename(fname))[0]\n",
        "\n",
        "            # Useful check.\n",
        "            assert key not in all_keys\n",
        "            all_keys.add(key)\n",
        "\n",
        "            # Construct a sample.\n",
        "            xkey = key if filekey else \"%07d\" % i\n",
        "            sample = {\"__key__\": xkey, \"jpg\": image, \"cls\": cls}\n",
        "\n",
        "            # Write the sample to the sharded tar archives.\n",
        "            sink.write(sample)\n",
        "\n",
        "for split in splits:\n",
        "    print(\"# split\", split)\n",
        "    write_dataset(data, base=shards, split=split)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWdSmBcLSiWP"
      },
      "source": [
        "# Timing the Read Access of WebDataset\n",
        "\n",
        "Now that the dataset has been sharded for WebDataset, we can start making the dataloaders to iterate over the dataset and time the read access overhead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WYkTeHXFHw4a"
      },
      "source": [
        "def time_webdataset(url, batch_size=64):\n",
        "    dataset = wds.Dataset(url)\n",
        "    loader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, num_workers=8)\n",
        "    start = time.time()\n",
        "    for filename, inputs, targets in loader:\n",
        "        x, y = inputs, targets\n",
        "    end = time.time()\n",
        "    print(\"Time taken by WebDataset: \", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PfUvvCsKS0Uv"
      },
      "source": [
        "# Timing the Read Access of Hub\n",
        "\n",
        "The following cell aims to time the package \"Hub\" for the same read access overhead."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS8LLMKLSzyf"
      },
      "source": [
        "def time_hub(tag, batch_size=64):\n",
        "    dataset = hub.Dataset(tag)\n",
        "    start = time.time()\n",
        "    for batch in range(dataset.shape[0] // batch_size):\n",
        "        x = dataset[\"image\"][batch * batch_size : (batch + 1) * batch_size].compute()\n",
        "        y = dataset[\"label\"][batch * batch_size : (batch + 1) * batch_size].compute()\n",
        "    if dataset.shape[0] % batch_size != 0:\n",
        "        x = dataset[\"image\"][(dataset.shape[0] // batch_size) * batch_size : ].compute()\n",
        "        y = dataset[\"label\"][(dataset.shape[0] // batch_size) * batch_size : ].compute()\n",
        "    end = time.time()\n",
        "    print(\"Time taken by Hub (no conversion): \", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZEGD4NFXb52"
      },
      "source": [
        "# Timing the Read Access of Hub converted to PyTorch\n",
        "\n",
        "Since WebDataset is based on PyTorch and Hub offers PyTorch integration, it would be useful to compare Hub's performance when converted to PyTorch as well."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MChib6OpWtNn"
      },
      "source": [
        "def time_hub_torch(tag, batch_size=64):\n",
        "    dataset = hub.Dataset(tag)\n",
        "    dataset = dataset.to_pytorch()\n",
        "    dataset = torch.utils.data.DataLoader(\n",
        "        dataset,\n",
        "        batch_size=batch_size\n",
        "    )\n",
        "    start = time.time()\n",
        "    for batch in dataset:\n",
        "        x = batch[\"image\"]\n",
        "        y = batch[\"label\"]\n",
        "    end = time.time()\n",
        "    print(\"Time taken by Hub converted to PyTorch: \", end - start)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MsCuq8OjY1tG"
      },
      "source": [
        "## Running the Experiment\n",
        "\n",
        "Now that we have all the utility functions, we define the parameters we want to test with and can run the functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RBA4mS0nWu0s"
      },
      "source": [
        "sharedurl = \"/shards/imagenet-train-{000000..001281}.tar\"             # Data for WebDataset, must be in torch.Datasets.ImageNet compatible format.\n",
        "tag = \"./data-hub\"                                                    # Data for Hub\n",
        "\n",
        "BATCH_SIZE = 1000                                                     # Batch Size for all the DataLoaders.  Can be changed."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5UCFqNt1ZNo-"
      },
      "source": [
        "time_webdataset(sharedurl, BATCH_SIZE)\n",
        "time_hub(tag, BATCH_SIZE)\n",
        "time_hub_torch(tag, BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}